{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e71d685"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow\n",
        "%pip install keras\n",
        "%pip install matplotlib\n",
        "%pip install seaborn\n",
        "%pip install numpy\n",
        "%pip install scikit-learn\n",
        "%pip install tqdm\n",
        "%pip install plotly\n",
        "%pip install opencv-python\n",
        "%pip install visualkeras"
      ],
      "id": "5e71d685"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "438b9258"
      },
      "outputs": [],
      "source": [
        "# Hardware configuration\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "id": "438b9258"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce37aa13"
      },
      "outputs": [],
      "source": [
        "# Importing Important libraries\n",
        "import os\n",
        "from keras.utils import image_dataset_from_directory\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "id": "ce37aa13"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0c75119"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.cm as cm\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tqdm.pandas()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "f0c75119"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3yZ59flpoT7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "H3yZ59flpoT7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a561073"
      },
      "outputs": [],
      "source": [
        "def get_path(plant_dir:str, dir_test:str):\n",
        "    if dir_test == 'Test':\n",
        "        return '/content/drive/MyDrive/data/' + plant_dir + '/Test'\n",
        "    elif dir_test == 'Train':\n",
        "        return '/content/drive/MyDrive/data/' + plant_dir + '/Train'\n",
        "    elif dir_test == 'Val':\n",
        "        return '/content/drive/MyDrive/data/' + plant_dir + '/Val'"
      ],
      "id": "1a561073"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c958bd0"
      },
      "outputs": [],
      "source": [
        "# No. of Directories containing images (data) for training, testing & validation\n",
        "plant_dirs = list(os.listdir('/content/drive/MyDrive/data'))\n",
        "plant_dirs"
      ],
      "id": "6c958bd0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3df0fd14"
      },
      "outputs": [],
      "source": [
        "# Image dimensionality and batch size\n",
        "image_dim = (256, 256)\n",
        "batch_size = 32\n",
        "num_channels = 3\n",
        "input_shape = (batch_size, image_dim[0], image_dim[1], num_channels)   # (32, 256, 256, 3)"
      ],
      "id": "3df0fd14"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2852440c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_dataset = {}\n",
        "print('================ Images & Classes for Training ================\\n')\n",
        "for plant in plant_dirs:\n",
        "    print('>>> No. of Images & Classes in \"{}\" directory'.format(plant))\n",
        "    train_dataset[plant] = image_dataset_from_directory(get_path(plant, 'Train'),\n",
        "                                                        shuffle = True,\n",
        "                                                        labels = 'inferred',\n",
        "                                                        label_mode = 'int',\n",
        "                                                        image_size = image_dim,\n",
        "                                                        batch_size = batch_size)\n",
        "    print (\"______________\\n\")"
      ],
      "id": "2852440c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35517f46-528e-489f-a2be-ab6747818ba6"
      },
      "outputs": [],
      "source": [
        "print(train_dataset)"
      ],
      "id": "35517f46-528e-489f-a2be-ab6747818ba6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb556060"
      },
      "outputs": [],
      "source": [
        "test_dataset = {}\n",
        "print('================ Images & Classes for Testing ================\\n')\n",
        "for plant in plant_dirs:\n",
        "    print('>>> No. of Images & Classes in \"{}\" directory'.format(plant))\n",
        "    test_dataset[plant] = image_dataset_from_directory(get_path(plant, 'Test'),\n",
        "                                                        shuffle = True,\n",
        "                                                        labels = 'inferred',\n",
        "                                                        label_mode = 'int',\n",
        "                                                        image_size = image_dim,\n",
        "                                                        batch_size = batch_size)\n",
        "    print (\"______________\\n\")"
      ],
      "id": "fb556060"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16b28b2b-2110-40c8-a48e-733b3ff36090"
      },
      "outputs": [],
      "source": [
        "print(test_dataset)"
      ],
      "id": "16b28b2b-2110-40c8-a48e-733b3ff36090"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffc9657f"
      },
      "outputs": [],
      "source": [
        "val_dataset = {}\n",
        "print('================ Images & Classes for Validation ================\\n')\n",
        "for plant in plant_dirs:\n",
        "    print('>>> No. of Images & Classes in \"{}\" directory'.format(plant))\n",
        "    val_dataset[plant] = image_dataset_from_directory(get_path(plant, 'Val'),\n",
        "                                                        shuffle = True,\n",
        "                                                        labels = 'inferred',\n",
        "                                                        label_mode = 'int',\n",
        "                                                        image_size = image_dim,\n",
        "                                                        batch_size = batch_size)\n",
        "    print (\"______________\\n\")"
      ],
      "id": "ffc9657f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d82a492a-4cf1-4540-be09-2516839790cd"
      },
      "outputs": [],
      "source": [
        "print(val_dataset)"
      ],
      "id": "d82a492a-4cf1-4540-be09-2516839790cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24d6900c"
      },
      "outputs": [],
      "source": [
        "classes = {}\n",
        "for plant in plant_dirs:\n",
        "    print('>>> Classes in {} dataset :-'.format(plant))\n",
        "    classes[plant] = []\n",
        "    for num,cat in enumerate(train_dataset[plant].class_names, start = 1):\n",
        "        classes[plant].append(cat)\n",
        "        print(num, cat)\n",
        "    print('\\n')"
      ],
      "id": "24d6900c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22afd763"
      },
      "outputs": [],
      "source": [
        "for plant in plant_dirs:\n",
        "    print('>>> Sample Images of \"{}\" dataset'.format(plant))\n",
        "    plt.figure(figsize = (14,5))\n",
        "    for image_batch, image_label in train_dataset[plant].take(1):\n",
        "        for i in range(10):\n",
        "            plt.subplot(2,5,i+1)\n",
        "            plt.imshow(image_batch[i].numpy().astype('uint8'))\n",
        "            plt.title(classes[plant][image_label[i]])\n",
        "            plt.axis('off')\n",
        "        plt.show()\n",
        "    print('\\n')"
      ],
      "id": "22afd763"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38d22deb"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots"
      ],
      "id": "38d22deb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca6af80c"
      },
      "outputs": [],
      "source": [
        "red_channel_values = []\n",
        "green_channel_values = []\n",
        "blue_channel_values = []\n",
        "\n",
        "for images, labels in train_dataset[plant]:\n",
        "        # Convert the images to numpy array\n",
        "        image_array = np.array(images)\n",
        "\n",
        "        # Extract the red, green, blue channel values\n",
        "        red_channel = [np.mean(image_array[:, :, :, 0])]\n",
        "        green_channel =[ np.mean(image_array[:,:,:,1])]\n",
        "        blue_channel = [np.mean(image_array[:,:,:,2])]\n",
        "\n",
        "        red_channel_values.extend(red_channel)\n",
        "        green_channel_values.extend(green_channel)\n",
        "        blue_channel_values.extend(blue_channel)"
      ],
      "id": "ca6af80c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44cc5809"
      },
      "outputs": [],
      "source": [
        "fig = ff.create_distplot([red_channel_values], group_labels=[\"R\"], colors=[\"red\"])\n",
        "fig.update_layout(showlegend=False, template=\"simple_white\")\n",
        "fig.update_layout(title_text=\"Distribution of red channel values\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig"
      ],
      "id": "44cc5809"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "781c343a"
      },
      "outputs": [],
      "source": [
        "fig = ff.create_distplot([green_channel_values], group_labels=[\"G\"], colors=[\"green\"])\n",
        "fig.update_layout(showlegend=False, template=\"simple_white\")\n",
        "fig.update_layout(title_text=\"Distribution of green channel values\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig"
      ],
      "id": "781c343a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef62ba68"
      },
      "outputs": [],
      "source": [
        "fig = ff.create_distplot([blue_channel_values], group_labels=[\"B\"], colors=[\"blue\"])\n",
        "fig.update_layout(showlegend=False, template=\"simple_white\")\n",
        "fig.update_layout(title_text=\"Distribution of blue channel values\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig"
      ],
      "id": "ef62ba68"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf0f70ea"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "for idx, values in enumerate([red_channel_values, green_channel_values, blue_channel_values]):\n",
        "    if idx == 0:\n",
        "        color = \"Red\"\n",
        "    if idx == 1:\n",
        "        color = \"Green\"\n",
        "    if idx == 2:\n",
        "        color = \"Blue\"\n",
        "    fig.add_trace(go.Box(x=[color]*len(values), y=values, name=color, marker=dict(color=color.lower())))\n",
        "\n",
        "fig.update_layout(yaxis_title=\"Mean value\", xaxis_title=\"Color channel\",\n",
        "                  title=\"Mean value vs. Color channel\", template=\"plotly_white\")\n"
      ],
      "id": "cf0f70ea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9ef09e5"
      },
      "outputs": [],
      "source": [
        "fig = ff.create_distplot([red_channel_values, green_channel_values, blue_channel_values],\n",
        "                         group_labels=[\"R\", \"G\", \"B\"],\n",
        "                         colors=[\"red\", \"green\", \"blue\"])\n",
        "fig.update_layout(title_text=\"Distribution of RGB channel values\", template=\"simple_white\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig.data[1].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[1].marker.line.width = 0.5\n",
        "fig.data[2].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[2].marker.line.width = 0.5\n",
        "fig"
      ],
      "id": "f9ef09e5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09a6d4cd"
      },
      "outputs": [],
      "source": [
        "class_names = train_dataset['Apple'].class_names\n",
        "\n",
        "class_counts = {class_name: 0 for class_name in class_names}\n",
        "\n",
        "for images, labels in train_dataset['Apple']:\n",
        "    unique_labels, _, counts = tf.unique_with_counts(labels)\n",
        "    for class_idx, count in zip(unique_labels.numpy(), counts.numpy()):\n",
        "        class_name = class_names[class_idx]\n",
        "        class_counts[class_name] += count\n",
        "\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"Class: {class_name}, Count: {count}\")\n",
        "\n",
        "counts_array = np.array([class_counts[class_name] for class_name in class_names])\n",
        "\n",
        "fig = go.Figure(data=[go.Pie(labels=class_names, values=counts_array)])\n",
        "fig.update_layout(title_text=\"Data Distribution for Apple\", template=\"simple_white\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig.show()"
      ],
      "id": "09a6d4cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "525230f9"
      },
      "outputs": [],
      "source": [
        "class_names = train_dataset['Tomato'].class_names\n",
        "\n",
        "class_counts = {class_name: 0 for class_name in class_names}\n",
        "\n",
        "for images, labels in train_dataset['Tomato']:\n",
        "    unique_labels, _, counts = tf.unique_with_counts(labels)\n",
        "    for class_idx, count in zip(unique_labels.numpy(), counts.numpy()):\n",
        "        class_name = class_names[class_idx]\n",
        "        class_counts[class_name] += count\n",
        "\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"Class: {class_name}, Count: {count}\")\n",
        "\n",
        "counts_array = np.array([class_counts[class_name] for class_name in class_names])\n",
        "\n",
        "fig = go.Figure(data=[go.Pie(labels=class_names, values=counts_array)])\n",
        "fig.update_layout(title_text=\"Data Distribution for Tomato\", template=\"simple_white\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig.show()"
      ],
      "id": "525230f9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5j2ih4j_JfY"
      },
      "outputs": [],
      "source": [
        "class_names = train_dataset['Corn (Maize)'].class_names\n",
        "\n",
        "class_counts = {class_name: 0 for class_name in class_names}\n",
        "\n",
        "for images, labels in train_dataset['Corn (Maize)']:\n",
        "    unique_labels, _, counts = tf.unique_with_counts(labels)\n",
        "    for class_idx, count in zip(unique_labels.numpy(), counts.numpy()):\n",
        "        class_name = class_names[class_idx]\n",
        "        class_counts[class_name] += count\n",
        "\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"Class: {class_name}, Count: {count}\")\n",
        "\n",
        "counts_array = np.array([class_counts[class_name] for class_name in class_names])\n",
        "\n",
        "fig = go.Figure(data=[go.Pie(labels=class_names, values=counts_array)])\n",
        "fig.update_layout(title_text=\"Data Distribution for Corn (Maize)\", template=\"simple_white\")\n",
        "fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n",
        "fig.data[0].marker.line.width = 0.5\n",
        "fig.show()"
      ],
      "id": "v5j2ih4j_JfY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c63ed095"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def edge_and_cut(img):\n",
        "    emb_img = np.array(img, dtype=np.uint8)  # Convert to NumPy array\n",
        "    img_np = np.array(img, dtype=np.uint8)  # Convert to uint8\n",
        "    edges = cv2.Canny(img_np, 100, 200)\n",
        "    edge_coors = []\n",
        "    for i in range(edges.shape[0]):\n",
        "        for j in range(edges.shape[1]):\n",
        "            if edges[i][j] != 0:\n",
        "                edge_coors.append((i, j))\n",
        "\n",
        "    row_min = edge_coors[np.argsort([coor[0] for coor in edge_coors])[0]][0]\n",
        "    row_max = edge_coors[np.argsort([coor[0] for coor in edge_coors])[-1]][0]\n",
        "    col_min = edge_coors[np.argsort([coor[1] for coor in edge_coors])[0]][1]\n",
        "    col_max = edge_coors[np.argsort([coor[1] for coor in edge_coors])[-1]][1]\n",
        "    new_img = img_np[row_min:row_max, col_min:col_max]\n",
        "\n",
        "    emb_img[row_min-10:row_min+10, col_min:col_max] = [255, 0, 0]\n",
        "    emb_img[row_max-10:row_max+10, col_min:col_max] = [255, 0, 0]\n",
        "    emb_img[row_min:row_max, col_min-10:col_min+10] = [255, 0, 0]\n",
        "    emb_img[row_min:row_max, col_max-10:col_max+10] = [255, 0, 0]\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n",
        "    ax[0].imshow(img_np)\n",
        "    ax[0].set_title('Original Image', fontsize=24)\n",
        "    ax[1].imshow(edges, cmap='gray')\n",
        "    ax[1].set_title('Canny Edges', fontsize=24)\n",
        "    ax[2].imshow(emb_img)\n",
        "    ax[2].set_title('Bounding Box', fontsize=24)\n",
        "    plt.show()"
      ],
      "id": "c63ed095"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04a3597a"
      },
      "outputs": [],
      "source": [
        "num_images = 5  # Number of images to select\n",
        "for images, _ in train_dataset['Apple']:\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        image = images[i]\n",
        "        edge_and_cut(image)\n",
        "        num_images -= 1\n",
        "        if num_images == 0:\n",
        "            break\n",
        "    if num_images == 0:\n",
        "        break"
      ],
      "id": "04a3597a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52621858"
      },
      "outputs": [],
      "source": [
        "num_images = 5  # Number of images to select\n",
        "for images, _ in train_dataset['Tomato']:\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        image = images[i]\n",
        "        edge_and_cut(image)\n",
        "        num_images -= 1\n",
        "        if num_images == 0:\n",
        "            break\n",
        "    if num_images == 0:\n",
        "        break"
      ],
      "id": "52621858"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68d05920"
      },
      "outputs": [],
      "source": [
        "num_images = 5  # Number of images to select\n",
        "for images, _ in train_dataset['Corn (Maize)']:\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        image = images[i]\n",
        "        edge_and_cut(image)\n",
        "        num_images -= 1\n",
        "        if num_images == 0:\n",
        "            break\n",
        "    if num_images == 0:\n",
        "        break"
      ],
      "id": "68d05920"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbb83469"
      },
      "outputs": [],
      "source": [
        "def invert(img):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n",
        "    img_np = np.array(img, dtype=np.uint8)  # Convert to NumPy array\n",
        "    ax[0].imshow(img_np)\n",
        "    ax[0].set_title('Original Image', fontsize=24)\n",
        "    ax[1].imshow(cv2.flip(img_np, 0))\n",
        "    ax[1].set_title('Vertical Flip', fontsize=24)\n",
        "    ax[2].imshow(cv2.flip(img_np, 1))\n",
        "    ax[2].set_title('Horizontal Flip', fontsize=24)\n",
        "    plt.show()"
      ],
      "id": "cbb83469"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30177c34"
      },
      "outputs": [],
      "source": [
        "num_images = 5  # Number of images to select\n",
        "for images, _ in train_dataset['Apple']:\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        image = images[i]\n",
        "        invert(image)\n",
        "        num_images -= 1\n",
        "        if num_images == 0:\n",
        "            break\n",
        "    if num_images == 0:\n",
        "        break"
      ],
      "id": "30177c34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1898410e-cb2b-46c8-a45d-aadab18857d9"
      },
      "outputs": [],
      "source": [
        "num_images = 5  # Number of images to select\n",
        "for images, _ in train_dataset['Corn (Maize)']:\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        image = images[i]\n",
        "        invert(image)\n",
        "        num_images -= 1\n",
        "        if num_images == 0:\n",
        "            break\n",
        "    if num_images == 0:\n",
        "        break"
      ],
      "id": "1898410e-cb2b-46c8-a45d-aadab18857d9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "342ec149-a99b-4753-b38d-e66e1eec2da8"
      },
      "outputs": [],
      "source": [
        "num_images = 5  # Number of images to select\n",
        "for images, _ in train_dataset['Tomato']:\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        image = images[i]\n",
        "        invert(image)\n",
        "        num_images -= 1\n",
        "        if num_images == 0:\n",
        "            break\n",
        "    if num_images == 0:\n",
        "        break"
      ],
      "id": "342ec149-a99b-4753-b38d-e66e1eec2da8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4968093"
      },
      "outputs": [],
      "source": [
        "def conv(img):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\n",
        "    img_np = np.array(img, dtype=np.uint8)  # Convert to NumPy array\n",
        "    kernel = np.ones((7, 7), np.float32)/25\n",
        "    conv = cv2.filter2D(img_np, -1, kernel)\n",
        "    ax[0].imshow(img_np)\n",
        "    ax[0].set_title('Original Image', fontsize=24)\n",
        "    ax[1].imshow(conv)\n",
        "    ax[1].set_title('Convolved Image', fontsize=24)\n",
        "    plt.show()"
      ],
      "id": "a4968093"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "347007a8"
      },
      "outputs": [],
      "source": [
        "num_images = 5  # Number of images to select\n",
        "for images, _ in train_dataset['Apple']:\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        image = images[i]\n",
        "        conv(image)\n",
        "        num_images -= 1\n",
        "        if num_images == 0:\n",
        "            break\n",
        "    if num_images == 0:\n",
        "        break"
      ],
      "id": "347007a8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c01f2c37-bdbc-49bf-9820-f9e1bf2e2c5b"
      },
      "outputs": [],
      "source": [
        "num_images = 5  # Number of images to select\n",
        "for images, _ in train_dataset['Corn (Maize)']:\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        image = images[i]\n",
        "        conv(image)\n",
        "        num_images -= 1\n",
        "        if num_images == 0:\n",
        "            break\n",
        "    if num_images == 0:\n",
        "        break"
      ],
      "id": "c01f2c37-bdbc-49bf-9820-f9e1bf2e2c5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5565bad-2ab0-4658-bf17-c6a12bfca3b4"
      },
      "outputs": [],
      "source": [
        "num_images = 5  # Number of images to select\n",
        "for images, _ in train_dataset['Tomato']:\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        image = images[i]\n",
        "        conv(image)\n",
        "        num_images -= 1\n",
        "        if num_images == 0:\n",
        "            break\n",
        "    if num_images == 0:\n",
        "        break"
      ],
      "id": "a5565bad-2ab0-4658-bf17-c6a12bfca3b4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f30ed3d"
      },
      "outputs": [],
      "source": [
        "def blur(img):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\n",
        "    img_np = np.array(img, dtype=np.uint8)  # Convert to NumPy array\n",
        "    ax[0].imshow(img_np)\n",
        "    ax[0].set_title('Original Image', fontsize=24)\n",
        "    ax[1].imshow(cv2.blur(img_np, (100, 100)))\n",
        "    ax[1].set_title('Blurred Image', fontsize=24)\n",
        "    plt.show()"
      ],
      "id": "2f30ed3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d7d92f9"
      },
      "outputs": [],
      "source": [
        "num_images = 5  # Number of images to select\n",
        "for images, _ in train_dataset['Apple']:\n",
        "    for i in range(min(num_images, len(images))):\n",
        "        image = images[i]\n",
        "        blur(image)\n",
        "        num_images -= 1\n",
        "        if num_images == 0:\n",
        "            break\n",
        "    if num_images == 0:\n",
        "        break"
      ],
      "id": "9d7d92f9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4363b4b9"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Dropout, Flatten, Input, LeakyReLU\n",
        "from keras.layers import BatchNormalization, Activation, Conv2D\n",
        "from keras.applications import ResNet101V2\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Flatten, MaxPooling2D, Dense, Dropout"
      ],
      "id": "4363b4b9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f700a4ab"
      },
      "outputs": [],
      "source": [
        "target_names = []\n",
        "for plant in train_dataset.keys():\n",
        "    for class_name in train_dataset[plant].class_names:\n",
        "        target_name = plant + '__' + class_name.replace(' ', '_')\n",
        "        target_names.append(target_name)\n",
        "\n",
        "print(target_names)"
      ],
      "id": "f700a4ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15d5b6b5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "training_data = tf.keras.preprocessing.image_dataset_from_directory(\"/content/drive/MyDrive/data/train/\",\n",
        "    seed=42,\n",
        "    image_size= (img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "id": "15d5b6b5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d546d68c"
      },
      "outputs": [],
      "source": [
        "validation_data = tf.keras.preprocessing.image_dataset_from_directory(\"/content/drive/MyDrive/data/val/\",\n",
        "    seed=42,\n",
        "    image_size= (img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "id": "d546d68c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dbb0acd"
      },
      "outputs": [],
      "source": [
        "from keras.applications import DenseNet121\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "\n",
        "# Load the DenseNet121 model\n",
        "base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom fully connected layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(4096, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "predictions = Dense(len(target_names), activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "denseNetModel = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "id": "8dbb0acd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac67a0c7-5de8-4242-b69f-7e773761e0e1"
      },
      "outputs": [],
      "source": [
        "import visualkeras"
      ],
      "id": "ac67a0c7-5de8-4242-b69f-7e773761e0e1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29cdeae0-baab-4258-8c79-42a9c8cf5849"
      },
      "outputs": [],
      "source": [
        "visualkeras.layered_view(denseNetModel, legend=True)"
      ],
      "id": "29cdeae0-baab-4258-8c79-42a9c8cf5849"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23d2acc7"
      },
      "outputs": [],
      "source": [
        "denseNetModel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "id": "23d2acc7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46f1e1ed"
      },
      "outputs": [],
      "source": [
        "from keras.applications import EfficientNetB0\n",
        "\n",
        "# Load the EfficientNetB0 model\n",
        "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom fully connected layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(4096, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "predictions = Dense(len(target_names), activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "efficientNetModel = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "id": "46f1e1ed"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e00edfa-fda5-4b4e-bfe4-e8feff444f10"
      },
      "outputs": [],
      "source": [
        "visualkeras.layered_view(efficientNetModel, legend=True)"
      ],
      "id": "4e00edfa-fda5-4b4e-bfe4-e8feff444f10"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "418be0a3"
      },
      "outputs": [],
      "source": [
        "denseNetModelTrain = denseNetModel.fit(train_data, validation_data=val_data, epochs=10)"
      ],
      "id": "418be0a3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c90e2f26-230b-410b-befa-3cca38156763"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
      ],
      "id": "c90e2f26-230b-410b-befa-3cca38156763"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1289623b-bc91-41ea-8b69-bc5a14982534"
      },
      "outputs": [],
      "source": [
        "test_images = []\n",
        "test_labels = []\n",
        "\n",
        "# Extract images and labels from train_dataset\n",
        "for plant in test_dataset.keys():\n",
        "    for image, label in test_dataset[plant]:\n",
        "        test_images.append(image)\n",
        "        test_labels.append(label)"
      ],
      "id": "1289623b-bc91-41ea-8b69-bc5a14982534"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d1b3c73-83f2-4e81-8e02-5e72fb346453"
      },
      "outputs": [],
      "source": [
        "test_images_data = tf.concat(test_images, axis=0)\n",
        "test_labels_data = tf.concat(test_labels, axis=0)"
      ],
      "id": "4d1b3c73-83f2-4e81-8e02-5e72fb346453"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fca738f3-2dc7-4838-b140-614f5277769d"
      },
      "outputs": [],
      "source": [
        "predictions = denseNetModel.predict(test_images_data)\n",
        "# Convert the predictions to class labels\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(test_labels_data, predicted_labels, average='weighted')\n",
        "recall = recall_score(test_labels_data, predicted_labels, average='weighted')\n",
        "f1 = f1_score(test_labels_data, predicted_labels, average='weighted')\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "confusion_mat = confusion_matrix(test_labels_data, predicted_labels)"
      ],
      "id": "fca738f3-2dc7-4838-b140-614f5277769d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80c3d662-c918-4d0e-b588-4cf4da9d4462"
      },
      "outputs": [],
      "source": [
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)"
      ],
      "id": "80c3d662-c918-4d0e-b588-4cf4da9d4462"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be18899d-f040-4ba8-9a3d-25cb212093c9"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_mat, annot=True)"
      ],
      "id": "be18899d-f040-4ba8-9a3d-25cb212093c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0601259e-8864-4fdf-afb3-93867b7a8023"
      },
      "outputs": [],
      "source": [
        "acc = denseNetModelTrain.history['accuracy']\n",
        "val_acc = denseNetModelTrain.history['val_accuracy']\n",
        "loss = denseNetModelTrain.history['loss']\n",
        "val_loss = denseNetModelTrain.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "#Train and validation accuracy\n",
        "plt.plot(epochs, acc, 'b', label='Training accurarcy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and Validation accurarcy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "#Train and validation loss\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "0601259e-8864-4fdf-afb3-93867b7a8023"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70ac8432-7c5a-4149-b237-f13681860db9"
      },
      "outputs": [],
      "source": [
        "efficientNetModel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "id": "70ac8432-7c5a-4149-b237-f13681860db9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21edae44"
      },
      "outputs": [],
      "source": [
        "efficientNetModelTrain = efficientNetModel.fit(train_data, validation_data=val_data, epochs=6)"
      ],
      "id": "21edae44"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20ea0a35-3e60-4891-b7ae-21078954182a"
      },
      "outputs": [],
      "source": [
        "predictions = efficientNetModel.predict(test_images_data)\n",
        "# Convert the predictions to class labels\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(test_labels_data, predicted_labels, average='weighted')\n",
        "recall = recall_score(test_labels_data, predicted_labels, average='weighted')\n",
        "f1 = f1_score(test_labels_data, predicted_labels, average='weighted')\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "confusion_mat = confusion_matrix(test_labels_data, predicted_labels)"
      ],
      "id": "20ea0a35-3e60-4891-b7ae-21078954182a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1973aeab-6ec9-4eec-9351-9e6c480dddd9"
      },
      "outputs": [],
      "source": [
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)"
      ],
      "id": "1973aeab-6ec9-4eec-9351-9e6c480dddd9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81054293-6900-4d01-a37b-c4806109e084"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_mat, annot=True)"
      ],
      "id": "81054293-6900-4d01-a37b-c4806109e084"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a53dfab9-45f6-4d0a-81fc-f1823965544d"
      },
      "outputs": [],
      "source": [
        "acc = efficientNetModelTrain.history['accuracy']\n",
        "val_acc = efficientNetModelTrain.history['val_accuracy']\n",
        "loss = efficientNetModelTrain.history['loss']\n",
        "val_loss = efficientNetModelTrain.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "#Train and validation accuracy\n",
        "plt.plot(epochs, acc, 'b', label='Training accurarcy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and Validation accurarcy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "#Train and validation loss\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "a53dfab9-45f6-4d0a-81fc-f1823965544d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d854ed9-3e94-41ab-a595-78efba02f9cf"
      },
      "outputs": [],
      "source": [
        "from keras.applications import ResNet50\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Load the pre-trained ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "# Add your own classification layers on top\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "predictions = Dense(len(target_names), activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "resnetModel = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "id": "4d854ed9-3e94-41ab-a595-78efba02f9cf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "187f40aa-941b-4ec7-9a7a-2c1552594c82"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "resnetModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "id": "187f40aa-941b-4ec7-9a7a-2c1552594c82"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c57e940-8844-4369-a031-a4e7bb4311e4"
      },
      "outputs": [],
      "source": [
        "visualkeras.layered_view(resnetModel, legend=True)"
      ],
      "id": "1c57e940-8844-4369-a031-a4e7bb4311e4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2507f44-aa11-44f7-8fbd-9f2d08716012"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "resnetModelTrain = resnetModel.fit(train_data, validation_data=val_data, epochs=5)"
      ],
      "id": "d2507f44-aa11-44f7-8fbd-9f2d08716012"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz1zMSKO4IaR"
      },
      "outputs": [],
      "source": [
        "from keras.applications import Xception\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "# Load the Xception model\n",
        "base_model = Xception(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom fully connected layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(4096, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "predictions = Dense(len(target_names), activation='softmax')(x)\n",
        "\n",
        "# Create the model\n",
        "xceptionModel = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "id": "yz1zMSKO4IaR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwjQta0S_ZS0"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "xceptionModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "id": "kwjQta0S_ZS0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir2EG2hF_ajX"
      },
      "outputs": [],
      "source": [
        "visualkeras.layered_view(xceptionModel, legend=True)"
      ],
      "id": "ir2EG2hF_ajX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8XHB5qe4Jbp"
      },
      "outputs": [],
      "source": [
        "xceptionModelTrain = xceptionModel.fit(train_data, validation_data=val_data, epochs=5)"
      ],
      "id": "k8XHB5qe4Jbp"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}